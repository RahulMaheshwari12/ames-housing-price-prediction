# ğŸ  Ames Housing Price Prediction using Linear & Ridge Regression

## ğŸ“Œ Project Overview
This project builds a **robust machine learning regression pipeline** to predict house prices using the **Ames Housing dataset**.  
The focus is not just on accuracy, but on **correct ML workflow**, including preprocessing, pipelines, regularization, and cross-validation.

A baseline **Linear Regression** model is first established, followed by a **Ridge Regression model with hyperparameter tuning** to handle multicollinearity and improve generalization.

---

## ğŸ¯ Objective
- Predict house sale prices accurately  
- Compare **baseline Linear Regression** with **regularized Ridge Regression**  
- Apply **industry-standard ML practices**:
  - Pipelines  
  - Proper trainâ€“test split  
  - Cross-validation  
  - Hyperparameter tuning  
  - Leakage-free preprocessing  

---

## ğŸ“‚ Dataset
- **Dataset**: Ames Housing Dataset  
- **Target variable**: `SalePrice`  
- **Features**:
  - Numerical: house size, quality, year built, etc.  
  - Categorical: neighborhood, exterior type, garage features, etc.  

The dataset contains **mixed feature types** and **high multicollinearity**, making it ideal for regularized linear models.

---

## ğŸ› ï¸ Tools & Libraries
- Python  
- Pandas, NumPy  
- Scikit-learn  
- Jupyter Notebook  

---

## âš™ï¸ Methodology & Workflow

### 1ï¸âƒ£ Exploratory Data Analysis (Minimal & ML-Focused)
- Checked data shape, types, and summary statistics  
- Identified numerical and categorical features  
- Analyzed missing values and categorical levels  
- Avoided unnecessary visual EDA  

---

### 2ï¸âƒ£ Data Preprocessing
- Categorical features handled using **One-Hot Encoding**  
- Numerical features scaled using **StandardScaler**  
- Preprocessing performed using **ColumnTransformer**  
- Entire preprocessing wrapped inside a **Pipeline** to avoid data leakage  

---

### 3ï¸âƒ£ Trainâ€“Test Split
- 80% training, 20% testing  
- Fixed `random_state` for reproducibility  
- Test set used **only once** for final evaluation  

---

### 4ï¸âƒ£ Baseline Model: Linear Regression
- Built using a full preprocessing pipeline  
- Purpose:
  - Sanity check  
  - Performance benchmark  

**Baseline Performance**
- **RÂ² â‰ˆ 0.89**  
- **RMSE â‰ˆ 28,000**  

This confirms correct preprocessing and strong linear relationships in the data.

---

### 5ï¸âƒ£ Final Model: Ridge Regression with Cross-Validation
- Ridge regression chosen to handle **multicollinearity**  
- Hyperparameter `alpha` tuned using **GridSearchCV (5-fold CV)**  
- Model selection based only on training data  

**Why Ridge?**
- Stabilizes coefficients  
- Reduces overfitting  
- Improves robustness with correlated features  

---

## ğŸ“Š Model Evaluation Metrics
- **RÂ² Score** â€“ goodness of fit  
- **RMSE** â€“ interpretable prediction error in price units  

Ridge regression achieved **comparable or slightly improved performance** compared to the baseline while offering better generalization.

---

## âœ… Final Conclusion
- Linear Regression provided a strong baseline  
- Ridge Regression was selected as the **final model** due to:
  - Regularization  
  - Cross-validated hyperparameter selection  
  - Better stability in the presence of multicollinearity  

> Even when accuracy gains are modest, Ridge Regression is preferred for real-world robustness.

---

## ğŸ“ Project Structure
```
â”œâ”€â”€ Ames_housing_model1.ipynb
â”œâ”€â”€ AmesHousing.csv
â”œâ”€â”€ README.md
```

---

## ğŸš€ Key Learnings
- Importance of **pipelines** in preventing data leakage  
- Correct usage of **GridSearchCV with pipelines**  
- Why regularization matters even when baseline models perform well  
- Proper evaluation workflow in supervised ML  

---

## ğŸ“Œ Future Improvements (Optional)
- Try ElasticNet for combined L1/L2 regularization  
- Analyze and interpret top Ridge coefficients  
- Add feature importance stability comparison  
- Deploy model using a simple API  

---

## ğŸ‘¤ Author
**Rahul Maheshwari**  
Aspiring Data Scientist / Machine Learning Engineer
